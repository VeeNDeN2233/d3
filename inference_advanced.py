
import argparse
import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Tuple, Any



import matplotlib
matplotlib.use('Agg')

import cv2
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import yaml

from models.anomaly_detector import AnomalyDetector
from models.autoencoder_advanced import BidirectionalLSTMAutoencoder
from utils.pose_processor import PoseProcessor
from video_processor import VideoProcessor

logging.basicConfig(
    level=logging.INFO,
    format="%(asctime)s - %(name)s - %(levelname)s - %(message)s",
)
logger = logging.getLogger(__name__)


def convert_numpy_types(obj: Any) -> Any:
    if isinstance(obj, np.integer):
        return int(obj)
    elif isinstance(obj, np.floating):
        return float(obj)
    elif isinstance(obj, np.bool_):
        return bool(obj)
    elif isinstance(obj, np.ndarray):
        return obj.tolist()
    elif isinstance(obj, dict):
        return {key: convert_numpy_types(value) for key, value in obj.items()}
    elif isinstance(obj, (list, tuple)):
        return [convert_numpy_types(item) for item in obj]
    elif isinstance(obj, (str, int, float, bool, type(None))):
        return obj
    else:

        try:
            return str(obj)
        except:
            return obj


def load_model_and_detector(
    checkpoint_path: Path, config: dict, device: torch.device, model_type: str = "bidir_lstm"
) -> Tuple[nn.Module, AnomalyDetector]:

    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ checkpoint Ğ´Ğ»Ñ Ğ¿Ğ¾Ğ»ÑƒÑ‡ĞµĞ½Ğ¸Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ¹ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸
    checkpoint = torch.load(checkpoint_path, map_location=device, weights_only=False)
    saved_config = checkpoint.get("config", {})
    
    # Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµĞ¼ Ğ¿Ğ°Ñ€Ğ°Ğ¼ĞµÑ‚Ñ€Ñ‹ Ğ¸Ğ· ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ½Ğ¾Ğ³Ğ¾ config, ĞµÑĞ»Ğ¸ Ğ¾Ğ½ ĞµÑÑ‚ÑŒ, Ğ¸Ğ½Ğ°Ñ‡Ğµ Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ config
    if saved_config and "model" in saved_config:
        model_config = saved_config["model"]
        logger.info(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ Ğ¸Ğ· checkpoint")
    else:
        model_config = config.get("model", {})
        logger.info(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ Ñ‚ĞµĞºÑƒÑ‰Ğ°Ñ ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ñ")
    
    if model_type == "bidir_lstm":
        # Ğ‘ĞµÑ€ĞµĞ¼ Ğ·Ğ½Ğ°Ñ‡ĞµĞ½Ğ¸Ñ Ğ½Ğ°Ğ¿Ñ€ÑĞ¼ÑƒÑ Ğ¸Ğ· model_config Ğ±ĞµĞ· fallback Ğ½Ğ° config
        encoder_sizes = model_config["encoder_hidden_sizes"] if "encoder_hidden_sizes" in model_config else [128, 64, 32]
        decoder_sizes = model_config["decoder_hidden_sizes"] if "decoder_hidden_sizes" in model_config else [64, 128, 75]
        latent_size_val = model_config["latent_size"] if "latent_size" in model_config else 32
        input_size_val = model_config["input_size"] if "input_size" in model_config else 75
        
        logger.info(f"Ğ¡Ğ¾Ğ·Ğ´Ğ°Ğ½Ğ¸Ğµ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸: encoder={encoder_sizes}, decoder={decoder_sizes}, latent={latent_size_val}")
        
        model = BidirectionalLSTMAutoencoder(
            input_size=input_size_val,
            sequence_length=config["pose"]["sequence_length"],
            encoder_hidden_sizes=encoder_sizes,
            decoder_hidden_sizes=decoder_sizes,
            latent_size=latent_size_val,
            num_attention_heads=4,
            dropout=model_config.get("encoder_dropout", 0.2),
        ).to(device)
    else:
        raise ValueError(f"ĞœĞ¾Ğ´ĞµĞ»ÑŒ {model_type} Ğ½Ğµ Ğ¿Ğ¾Ğ´Ğ´ĞµÑ€Ğ¶Ğ¸Ğ²Ğ°ĞµÑ‚ÑÑ")
    
    # Ğ—Ğ°Ğ³Ñ€ÑƒĞ¶Ğ°ĞµĞ¼ Ğ²ĞµÑĞ° Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸
    model.load_state_dict(checkpoint["model_state_dict"])
    model.eval()
    
    logger.info(f"ĞœĞ¾Ğ´ĞµĞ»ÑŒ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½Ğ° Ğ¸Ğ· {checkpoint_path}")
    

    detector_path = checkpoint_path.parent / "anomaly_detector_advanced.pt"
    if not detector_path.exists():
        raise FileNotFoundError(f"Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½: {detector_path}")
    
    detector = AnomalyDetector.load(detector_path, model, device)
    
    logger.info(f"Ğ”ĞµÑ‚ĞµĞºÑ‚Ğ¾Ñ€ Ğ·Ğ°Ğ³Ñ€ÑƒĞ¶ĞµĞ½ Ğ¸Ğ· {detector_path}")
    logger.info(f"ĞŸĞ¾Ñ€Ğ¾Ğ³ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸: {detector.threshold:.6f}")
    
    return model, detector


def process_video(
    video_path: Path,
    video_processor: VideoProcessor,
    pose_processor: PoseProcessor,
    detector: AnomalyDetector,
    config: dict,
) -> Tuple[List[np.ndarray], List[float], List[bool], np.ndarray]:
    logger.info(f"ĞĞ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ° Ğ²Ğ¸Ğ´ĞµĞ¾: {video_path}")
    
    temp_output = video_path.parent / f"temp_{video_path.name}"
    
    result = video_processor._process_video_sync(
        str(video_path), str(temp_output), save_keypoints=True
    )
    
    if not result["success"]:
        raise RuntimeError(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ¾Ğ±Ñ€Ğ°Ğ±Ğ¾Ñ‚ĞºĞ¸ Ğ²Ğ¸Ğ´ĞµĞ¾: {result.get('error')}")
    

    keypoints_path = Path(result["keypoints_path"]) / "keypoints.json"
    with open(keypoints_path, "r", encoding="utf-8") as f:
        keypoints_data = json.load(f)
    

    keypoints_list = []
    for frame_data in keypoints_data["frames"]:
        landmarks = frame_data.get("landmarks")

        if landmarks and len(landmarks) == 33:
            kp = np.array(
                [[lm["x"], lm["y"], lm["z"], lm.get("visibility", 0.0)] for lm in landmarks],
                dtype=np.float32,
            )
        else:

            kp = np.zeros((33, 4), dtype=np.float32)
        
        keypoints_list.append(kp)
    

    sequences = pose_processor.process_keypoints(keypoints_list)
    
    if len(sequences) == 0:
        logger.warning("ĞĞµÑ‚ Ğ²Ğ°Ğ»Ğ¸Ğ´Ğ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹")
        return keypoints_list, [], [], np.array([])
    

    flattened_sequences = []
    for seq in sequences:
        flattened_sequences.append(pose_processor.flatten_sequence(seq))
    sequences_array = np.array(flattened_sequences, dtype=np.float32)
    sequences_tensor = torch.FloatTensor(sequences_array)
    

    is_anomaly, errors = detector.predict(sequences_tensor.to(detector.device))
    

    if temp_output.exists():
        temp_output.unlink()
    
    return keypoints_list, errors.tolist(), is_anomaly.tolist(), sequences_array


def visualize_results(
    errors: List[float],
    is_anomaly: List[bool],
    output_dir: Path,
    video_name: str,
    threshold: Optional[float] = None,
) -> Dict[str, Path]:
    output_dir.mkdir(parents=True, exist_ok=True)
    

    fig, ax = plt.subplots(figsize=(12, 6))
    frames = np.arange(len(errors))
    ax.plot(frames, errors, label="Reconstruction Error", linewidth=2, color="blue")
    

    if threshold is not None:
        ax.axhline(y=threshold, color="r", linestyle="--", label=f"Anomaly Threshold ({threshold:.4f})", linewidth=2)
    

    anomaly_frames = [i for i, is_anom in enumerate(is_anomaly) if is_anom]
    if anomaly_frames:
        ax.scatter(anomaly_frames, [errors[i] for i in anomaly_frames], 
                  color="red", s=50, label="Anomalous", zorder=5)
    
    ax.set_xlabel("Sequence Index", fontsize=12)
    ax.set_ylabel("Reconstruction Error (MSE)", fontsize=12)
    ax.set_title(f"Reconstruction Error Over Time - {video_name}", fontsize=14)
    ax.legend()
    ax.grid(True, alpha=0.3)
    
    error_plot_path = output_dir / "reconstruction_error.png"
    plt.savefig(error_plot_path, dpi=300, bbox_inches="tight")
    plt.close()
    
    logger.info(f"Ğ“Ñ€Ğ°Ñ„Ğ¸Ğº ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {error_plot_path}")
    
    return {"error_plot": error_plot_path}


def generate_report(
    video_path: Path,
    errors: List[float],
    is_anomaly: List[bool],
    detector: AnomalyDetector,
    output_dir: Path,
    age_weeks: Optional[float] = None,
    gestational_age_weeks: Optional[float] = None,
    sequences_array: Optional[np.ndarray] = None,
) -> Dict:
    if len(errors) == 0:
        return {}
    
    errors_array = np.array(errors)
    anomaly_rate = np.mean(is_anomaly) * 100
    anomalous_count = sum(is_anomaly)
    total_count = len(is_anomaly)
    

    mean_error = float(errors_array.mean())
    max_error = float(errors_array.max())
    

    # ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ÑĞµĞ¼ Ñ€Ğ¸ÑĞº Ğ½Ğ° Ğ¾ÑĞ½Ğ¾Ğ²Ğµ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹ Ğ˜ Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½Ğ¾Ğ¹ Ğ¾ÑˆĞ¸Ğ±ĞºĞ¸
    # Ğ›Ğ¾Ğ³Ğ¸ĞºĞ°: Ğ¿Ñ€Ğ¾Ğ²ĞµÑ€ÑĞµĞ¼ ÑĞ½Ğ°Ñ‡Ğ°Ğ»Ğ° Ğ¼Ğ°ĞºÑĞ¸Ğ¼Ğ°Ğ»ÑŒĞ½ÑƒÑ Ğ¾ÑˆĞ¸Ğ±ĞºÑƒ (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¿Ğ¸ĞºĞ¸), Ğ·Ğ°Ñ‚ĞµĞ¼ Ğ¿Ñ€Ğ¾Ñ†ĞµĞ½Ñ‚ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹
    logger.info(f"ĞĞ¿Ñ€ĞµĞ´ĞµĞ»ĞµĞ½Ğ¸Ğµ Ñ€Ğ¸ÑĞºĞ°: anomaly_rate={anomaly_rate:.2f}%, max_error={max_error:.6f}, mean_error={mean_error:.6f}, threshold={detector.threshold:.6f}")
    
    if max_error > detector.threshold * 2.0 or anomaly_rate > 30.0:
        risk_level = "high"
        gma_assessment = "ĞĞĞĞœĞĞ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ"
        cp_risk = "Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ Ñ€Ğ¸ÑĞº Ñ†ĞµÑ€ĞµĞ±Ñ€Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ¿Ğ°Ñ€Ğ°Ğ»Ğ¸Ñ‡Ğ°"
        logger.info(f"Ğ’Ñ‹ÑĞ¾ĞºĞ¸Ğ¹ Ñ€Ğ¸ÑĞº: max_error={max_error:.6f} > threshold*2={detector.threshold*2.0:.6f} Ğ¸Ğ»Ğ¸ anomaly_rate={anomaly_rate:.2f}% > 30%")
    elif max_error > detector.threshold * 1.5 or anomaly_rate > 15.0 or mean_error > detector.threshold:
        risk_level = "medium"
        gma_assessment = "ĞŸĞĞ”ĞĞ—Ğ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ"
        cp_risk = "Ğ£ĞœĞ•Ğ Ğ•ĞĞĞ«Ğ™ Ñ€Ğ¸ÑĞº Ğ½ĞµĞ²Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ¸Ñ‡ĞµÑĞºĞ¸Ñ… Ğ½Ğ°Ñ€ÑƒÑˆĞµĞ½Ğ¸Ğ¹"
        logger.info(f"Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€Ğ¸ÑĞº: max_error={max_error:.6f} > threshold*1.5={detector.threshold*1.5:.6f} Ğ¸Ğ»Ğ¸ anomaly_rate={anomaly_rate:.2f}% > 15% Ğ¸Ğ»Ğ¸ mean_error={mean_error:.6f} > threshold={detector.threshold:.6f}")
    elif anomaly_rate > 5.0 or mean_error > detector.threshold * 0.8 or anomalous_count > 10:
        risk_level = "medium"
        gma_assessment = "ĞŸĞĞ”ĞĞ—Ğ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ (Ğ»ĞµĞ³ĞºĞ¸Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ)"
        cp_risk = "ĞĞ˜Ğ—ĞšĞ˜Ğ™-Ğ£ĞœĞ•Ğ Ğ•ĞĞĞ«Ğ™ Ñ€Ğ¸ÑĞº"
        logger.info(f"Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€Ğ¸ÑĞº (Ğ»ĞµĞ³ĞºĞ¸Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ): anomaly_rate={anomaly_rate:.2f}% > 5% Ğ¸Ğ»Ğ¸ mean_error={mean_error:.6f} > threshold*0.8={detector.threshold*0.8:.6f} Ğ¸Ğ»Ğ¸ anomalous_count={anomalous_count} > 10")
    elif anomalous_count > 0:
        risk_level = "medium"
        gma_assessment = "ĞŸĞĞ”ĞĞ—Ğ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ (Ğ¾Ğ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸)"
        cp_risk = "ĞĞ˜Ğ—ĞšĞ˜Ğ™-Ğ£ĞœĞ•Ğ Ğ•ĞĞĞ«Ğ™ Ñ€Ğ¸ÑĞº (Ñ‚Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ)"
        logger.info(f"Ğ¡Ñ€ĞµĞ´Ğ½Ğ¸Ğ¹ Ñ€Ğ¸ÑĞº (Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¸): anomalous_count={anomalous_count} > 0")
    else:
        risk_level = "low"
        gma_assessment = "ĞĞĞ ĞœĞĞ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ"
        cp_risk = "ĞĞ˜Ğ—ĞšĞ˜Ğ™ Ñ€Ğ¸ÑĞº"
        logger.info(f"ĞĞ¸Ğ·ĞºĞ¸Ğ¹ Ñ€Ğ¸ÑĞº: Ğ²ÑĞµ Ğ¼ĞµÑ‚Ñ€Ğ¸ĞºĞ¸ Ğ² Ğ½Ğ¾Ñ€Ğ¼Ğµ")
    



    detailed_analysis = {}
    logger.info(f"ĞŸÑ€Ğ¾Ğ²ĞµÑ€ĞºĞ° sequences_array Ğ´Ğ»Ñ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: sequences_array is None={sequences_array is None}, "
               f"len={len(sequences_array) if sequences_array is not None else 0}, shape={sequences_array.shape if sequences_array is not None and hasattr(sequences_array, 'shape') else 'N/A'}")
    
    if sequences_array is not None and len(sequences_array) > 0:
        try:
            from utils.anomaly_analyzer import analyze_joint_errors
            from utils.normal_statistics import get_normal_statistics
            
            sequences_np = np.array(sequences_array)
            errors_np = np.array(errors)
            
            logger.info(f"Ğ—Ğ°Ğ¿ÑƒÑĞº Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {len(sequences_np)} Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹, {sum(is_anomaly)} Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ…")

            normal_statistics = get_normal_statistics()
            if normal_statistics:
                logger.info("Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒÑÑ‚ÑÑ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ· Ñ‚Ñ€ĞµĞ½Ğ¸Ñ€Ğ¾Ğ²Ğ¾Ñ‡Ğ½Ñ‹Ñ… Ğ´Ğ°Ğ½Ğ½Ñ‹Ñ…")
            else:
                logger.warning("ĞĞ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ğµ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ½Ğµ Ğ½Ğ°Ğ¹Ğ´ĞµĞ½Ñ‹, Ğ±ÑƒĞ´ÑƒÑ‚ Ğ¸ÑĞ¿Ğ¾Ğ»ÑŒĞ·Ğ¾Ğ²Ğ°Ñ‚ÑŒÑÑ ÑÑ‚Ğ°Ñ‚Ğ¸ÑÑ‚Ğ¸ĞºĞ¸ Ğ¸Ğ· Ñ‚ĞµĞºÑƒÑ‰ĞµĞ³Ğ¾ Ğ²Ğ¸Ğ´ĞµĞ¾")


            detailed_analysis = analyze_joint_errors(
                sequences_np,
                errors_np,
                detector.threshold,
                normal_statistics=normal_statistics,
                age_weeks=age_weeks,
                analyze_all_sequences=True
            )
            
            logger.info(f"Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ·Ğ°Ğ²ĞµÑ€ÑˆĞµĞ½: has_anomalies={detailed_analysis.get('has_anomalies', False)}, "
                       f"joint_findings={len(detailed_analysis.get('joint_analysis', {}).get('findings', []))}, "
                       f"asymmetry={detailed_analysis.get('asymmetry', {}).get('has_asymmetry', False)}")
            

            amplitude_analysis = detailed_analysis.get("amplitude_analysis", {})
            if amplitude_analysis.get("has_amplitude_anomalies", False):

                if amplitude_analysis.get("critical_amplitude_drop", False):
                    if risk_level == "low":
                        risk_level = "high"
                        anomaly_rate = 100.0
                        gma_assessment = "ĞĞĞĞœĞĞ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ (ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚Ğ¸)"
                        cp_risk = "Ğ’Ğ«Ğ¡ĞĞšĞ˜Ğ™ Ñ€Ğ¸ÑĞº (Ğ¾Ñ‚ÑÑƒÑ‚ÑÑ‚Ğ²Ğ¸Ğµ/ĞºÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¾Ğµ ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹)"
                    elif risk_level == "medium":
                        risk_level = "high"
                elif amplitude_analysis.get("moderate_amplitude_drop", False):
                    if risk_level == "low":
                        risk_level = "medium"
                        anomaly_rate = max(anomaly_rate, 50.0)
                        if gma_assessment == "ĞĞĞ ĞœĞĞ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ":
                            gma_assessment = "ĞŸĞĞ”ĞĞ—Ğ Ğ˜Ğ¢Ğ•Ğ›Ğ¬ĞĞ«Ğ• Ğ¾Ğ±Ñ‰Ğ¸Ğµ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ñ (ÑĞ½Ğ¸Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ°ĞºÑ‚Ğ¸Ğ²Ğ½Ğ¾ÑÑ‚ÑŒ)"
        except Exception as e:
            logger.error(f"ĞÑˆĞ¸Ğ±ĞºĞ° Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ğ¾Ğ³Ğ¾ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ·Ğ°: {e}", exc_info=True)
            detailed_analysis = {}
    else:
        logger.warning(f"Ğ”ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½ĞµĞ½: sequences_array is None Ğ¸Ğ»Ğ¸ Ğ¿ÑƒÑÑ‚Ğ¾Ğ¹ (None={sequences_array is None}, len={len(sequences_array) if sequences_array is not None else 0})")
    

    detected_signs = []
    if detailed_analysis.get("has_anomalies", False):

        asymmetry = detailed_analysis.get("asymmetry", {})
        if asymmetry.get("has_asymmetry", False):
            for finding in asymmetry.get("findings", []):
                detected_signs.append(finding["description"])
        

        joint_analysis = detailed_analysis.get("joint_analysis", {})
        for finding in joint_analysis.get("findings", []):
            if finding["type"] == "reduced_movement":
                detected_signs.append(finding["description"])
            elif finding["type"] == "high_speed":
                detected_signs.append(finding["description"])
        

        speed_analysis = detailed_analysis.get("speed_analysis", {})
        for finding in speed_analysis.get("findings", []):
            detected_signs.append(finding["description"])
        

        amplitude_analysis = detailed_analysis.get("amplitude_analysis", {})
        for finding in amplitude_analysis.get("findings", []):
            detected_signs.append(finding["description"])
    

    # Ğ”Ğ°Ğ¶Ğµ ĞµÑĞ»Ğ¸ Ğ´ĞµÑ‚Ğ°Ğ»ÑŒĞ½Ñ‹Ğ¹ Ğ°Ğ½Ğ°Ğ»Ğ¸Ğ· Ğ½Ğµ Ğ²Ñ‹Ğ¿Ğ¾Ğ»Ğ½Ğ¸Ğ»ÑÑ, Ğ´Ğ¾Ğ±Ğ°Ğ²Ğ»ÑĞµĞ¼ Ğ¸Ğ½Ñ„Ğ¾Ñ€Ğ¼Ğ°Ñ†Ğ¸Ñ Ğ¾Ğ± Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸ÑÑ…
    if len(detected_signs) == 0 and anomalous_count > 0:
        if anomaly_rate > 30:
            detected_signs.append(f"Ğ’Ñ‹ÑĞ¾ĞºĞ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ({anomaly_rate:.1f}% Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹)")
        elif anomaly_rate > 15:
            detected_signs.append(f"ĞŸĞ¾Ğ²Ñ‹ÑˆĞµĞ½Ğ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ({anomaly_rate:.1f}% Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹)")
        elif anomaly_rate > 5:
            detected_signs.append(f"Ğ£Ğ¼ĞµÑ€ĞµĞ½Ğ½Ğ°Ñ Ñ‡Ğ°ÑÑ‚Ğ¾Ñ‚Ğ° Ğ°Ğ½Ğ¾Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ({anomaly_rate:.1f}% Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹)")
        
        if max_error > detector.threshold * 2.0:
            detected_signs.append(f"ĞšÑ€Ğ¸Ñ‚Ğ¸Ñ‡ĞµÑĞºĞ¸Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… (Ğ¼Ğ°ĞºÑ. Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {max_error:.4f})")
        elif max_error > detector.threshold * 1.5:
            detected_signs.append(f"Ğ—Ğ½Ğ°Ñ‡Ğ¸Ñ‚ĞµĞ»ÑŒĞ½Ñ‹Ğµ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ² Ğ¾Ñ‚Ğ´ĞµĞ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑÑ… (Ğ¼Ğ°ĞºÑ. Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {max_error:.4f})")
        
        if mean_error > detector.threshold * 1.2:
            detected_signs.append("Ğ¡Ğ½Ğ¸Ğ¶ĞµĞ½Ğ½Ğ°Ñ Ğ²Ğ°Ñ€Ğ¸Ğ°Ğ±ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ÑŒ Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹")
        
        if len(detected_signs) == 0 and risk_level != "low":
            detected_signs.append(f"ĞĞ±Ğ½Ğ°Ñ€ÑƒĞ¶ĞµĞ½Ñ‹ Ğ¾Ñ‚ĞºĞ»Ğ¾Ğ½ĞµĞ½Ğ¸Ñ Ğ¾Ñ‚ Ğ½Ğ¾Ñ€Ğ¼Ğ°Ğ»ÑŒĞ½Ñ‹Ñ… Ğ¿Ğ°Ñ‚Ñ‚ĞµÑ€Ğ½Ğ¾Ğ² Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ ({anomalous_count} Ğ¸Ğ· {total_count} Ğ¿Ğ¾ÑĞ»ĞµĞ´Ğ¾Ğ²Ğ°Ñ‚ĞµĞ»ÑŒĞ½Ğ¾ÑÑ‚ĞµĞ¹)")
    

    recommendations = []
    if risk_level == "low":
        recommendations.append("âœ… Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ: ĞŸĞ»Ğ°Ğ½Ğ¾Ğ²Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ğ² 4 Ğ¼ĞµÑÑÑ†Ğ°")
        recommendations.append("ĞŸÑ€Ğ¾Ğ´Ğ¾Ğ»Ğ¶Ğ¸Ñ‚ÑŒ ÑÑ‚Ğ°Ğ½Ğ´Ğ°Ñ€Ñ‚Ğ½Ğ¾Ğµ Ğ½Ğ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ")
    elif risk_level == "medium":
        recommendations.append("âš ï¸ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ: ĞŸĞ¾Ğ²Ñ‚Ğ¾Ñ€Ğ½Ğ°Ñ Ğ¾Ñ†ĞµĞ½ĞºĞ° Ñ‡ĞµÑ€ĞµĞ· 2-4 Ğ½ĞµĞ´ĞµĞ»Ğ¸")
        recommendations.append("ĞĞ°Ğ±Ğ»ÑĞ´ĞµĞ½Ğ¸Ğµ Ñƒ Ğ¿ĞµĞ´Ğ¸Ğ°Ñ‚Ñ€Ğ°")
    else:
        recommendations.append("ğŸ”´ Ğ ĞµĞºĞ¾Ğ¼ĞµĞ½Ğ´Ğ°Ñ†Ğ¸Ñ: Ğ¡Ğ ĞĞ§ĞĞĞ¯ ĞºĞ¾Ğ½ÑÑƒĞ»ÑŒÑ‚Ğ°Ñ†Ğ¸Ñ Ğ´ĞµÑ‚ÑĞºĞ¾Ğ³Ğ¾ Ğ½ĞµĞ²Ñ€Ğ¾Ğ»Ğ¾Ğ³Ğ°")
        recommendations.append("ĞĞ°Ñ‡Ğ°Ñ‚ÑŒ Ñ€Ğ°Ğ½Ğ½ĞµĞµ Ğ²Ğ¼ĞµÑˆĞ°Ñ‚ĞµĞ»ÑŒÑÑ‚Ğ²Ğ¾")
        if detected_signs:
            recommendations.append(f"Ğ’Ñ‹ÑĞ²Ğ»ĞµĞ½Ñ‹ Ğ¿Ñ€Ğ¸Ğ·Ğ½Ğ°ĞºĞ¸: {', '.join(detected_signs)}")
    

    age_info = {}
    if age_weeks is not None:
        age_info["age_weeks"] = float(age_weeks)
        if age_weeks >= 9 and age_weeks <= 20:
            age_info["period"] = "ĞŸĞµÑ€Ğ¸Ğ¾Ğ´ ÑÑƒĞµÑ‚Ğ»Ğ¸Ğ²Ñ‹Ñ… Ğ´Ğ²Ğ¸Ğ¶ĞµĞ½Ğ¸Ğ¹ (fidgety movements)"
        elif age_weeks < 9:
            age_info["period"] = "Ğ Ğ°Ğ½Ğ½Ğ¸Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´ (writhing movements)"
        else:
            age_info["period"] = "ĞŸĞ¾Ğ·Ğ´Ğ½Ğ¸Ğ¹ Ğ¿ĞµÑ€Ğ¸Ğ¾Ğ´"
    
    if gestational_age_weeks is not None:
        age_info["gestational_age_weeks"] = float(gestational_age_weeks)
        if gestational_age_weeks < 37:
            age_info["premature"] = True
            age_info["corrected_age"] = age_weeks - (40 - gestational_age_weeks) if age_weeks else None
    
    report = {
        "video_path": str(video_path),
        "analysis_date": str(Path.cwd()),
        "gma_assessment": {
            "assessment_result": gma_assessment,
            "risk_level": risk_level.upper(),
            "cp_risk": cp_risk,
            "detected_signs": detected_signs,
        },
        "patient_info": age_info,
        "statistics": {
            "total_sequences": len(errors),
            "anomalous_sequences": sum(is_anomaly),
            "anomaly_rate": anomaly_rate,
        },
        "reconstruction_errors": {
            "mean": float(errors_array.mean()),
            "max": float(errors_array.max()),
            "min": float(errors_array.min()),
            "std": float(errors_array.std()),
        },
        "anomaly_detection": {
            "threshold": float(detector.threshold),
            "mean_anomaly_score": mean_error,
            "risk_level": risk_level,
            "anomaly_rate_percent": anomaly_rate,
        },
        "recommendations": recommendations,
        "detailed_analysis": detailed_analysis,
    }
    

    report_serializable = convert_numpy_types(report)
    

    report_path = output_dir / "medical_report.json"
    with open(report_path, "w", encoding="utf-8") as f:
        json.dump(report_serializable, f, indent=2, ensure_ascii=False)
    
    logger.info(f"ĞÑ‚Ñ‡ĞµÑ‚ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½: {report_path}")
    
    return report_serializable


def main():
    parser = argparse.ArgumentParser(description="Ğ˜Ğ½Ñ„ĞµÑ€ĞµĞ½Ñ Ğ´Ğ»Ñ Ğ¿Ñ€Ğ¾Ğ´Ğ²Ğ¸Ğ½ÑƒÑ‚Ğ¾Ğ¹ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
    parser.add_argument("--video", type=str, required=True, help="ĞŸÑƒÑ‚ÑŒ Ğº Ğ²Ğ¸Ğ´ĞµĞ¾")
    parser.add_argument("--checkpoint", type=str, default="checkpoints/best_model_advanced.pt",
                       help="ĞŸÑƒÑ‚ÑŒ Ğº checkpoint")
    parser.add_argument("--config", type=str, default="config.yaml", help="ĞŸÑƒÑ‚ÑŒ Ğº ĞºĞ¾Ğ½Ñ„Ğ¸Ğ³ÑƒÑ€Ğ°Ñ†Ğ¸Ğ¸")
    parser.add_argument("--output", type=str, help="ĞŸÑƒÑ‚ÑŒ Ğ´Ğ»Ñ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ğ¸Ñ Ñ€ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ğ¾Ğ²")
    parser.add_argument("--save_report", action="store_true", help="Ğ¡Ğ¾Ñ…Ñ€Ğ°Ğ½Ğ¸Ñ‚ÑŒ Ğ¾Ñ‚Ñ‡ĞµÑ‚")
    parser.add_argument("--model_type", type=str, default="bidir_lstm", 
                       choices=["bidir_lstm", "transformer"], help="Ğ¢Ğ¸Ğ¿ Ğ¼Ğ¾Ğ´ĞµĞ»Ğ¸")
    
    args = parser.parse_args()
    

    with open(args.config, "r", encoding="utf-8") as f:
        config = yaml.safe_load(f)
    

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type != "cuda":
        raise RuntimeError("Ğ¢Ñ€ĞµĞ±ÑƒĞµÑ‚ÑÑ GPU Ğ´Ğ»Ñ Ğ¸Ğ½Ñ„ĞµÑ€ĞµĞ½ÑĞ°!")
    
    logger.info(f"Ğ˜ÑĞ¿Ğ¾Ğ»ÑŒĞ·ÑƒĞµÑ‚ÑÑ ÑƒÑÑ‚Ñ€Ğ¾Ğ¹ÑÑ‚Ğ²Ğ¾: {device}")
    

    checkpoint_path = Path(args.checkpoint)
    model, detector = load_model_and_detector(checkpoint_path, config, device, args.model_type)
    

    video_processor = VideoProcessor(
        model_complexity=config["pose"]["model_complexity"],
        min_detection_confidence=config["pose"]["min_detection_confidence"],
        min_tracking_confidence=config["pose"]["min_tracking_confidence"],
    )
    
    pose_processor = PoseProcessor(
        sequence_length=config["pose"]["sequence_length"],
        sequence_stride=config["pose"]["sequence_stride"],
        normalize=config["pose"]["normalize"],
        normalize_relative_to=config["pose"]["normalize_relative_to"],
        target_hip_distance=config["pose"].get("target_hip_distance"),
        normalize_by_body=config["pose"].get("normalize_by_body", False),
        rotate_to_canonical=config["pose"].get("rotate_to_canonical", False),
    )
    

    video_path = Path(args.video)
    keypoints_list, errors, is_anomaly = process_video(
        video_path, video_processor, pose_processor, detector, config
    )
    

    if args.output:
        output_dir = Path(args.output)
    else:
        output_dir = Path("results") / video_path.stem
    
    output_dir.mkdir(parents=True, exist_ok=True)
    

    visualize_results(errors, is_anomaly, output_dir, video_path.stem, detector.threshold)
    

    if args.save_report:
        report = generate_report(video_path, errors, is_anomaly, detector, output_dir)
        
        logger.info("=" * 60)
        logger.info("Ğ Ğ•Ğ—Ğ£Ğ›Ğ¬Ğ¢ĞĞ¢Ğ« ĞĞĞĞ›Ğ˜Ğ—Ğ")
        logger.info("=" * 60)
        logger.info(f"Ğ£Ñ€Ğ¾Ğ²ĞµĞ½ÑŒ Ñ€Ğ¸ÑĞºĞ°: {report['anomaly_detection']['risk_level'].upper()}")
        logger.info(f"ĞĞ½Ğ¾Ğ¼Ğ°Ğ»Ğ¸Ğ¹: {report['anomaly_detection']['anomaly_rate_percent']:.2f}%")
        logger.info(f"Ğ¡Ñ€ĞµĞ´Ğ½ÑÑ Ğ¾ÑˆĞ¸Ğ±ĞºĞ°: {report['reconstruction_errors']['mean']:.6f}")
        logger.info(f"ĞŸĞ¾Ñ€Ğ¾Ğ³: {report['anomaly_detection']['threshold']:.6f}")
    
    logger.info(f"Ğ ĞµĞ·ÑƒĞ»ÑŒÑ‚Ğ°Ñ‚Ñ‹ ÑĞ¾Ñ…Ñ€Ğ°Ğ½ĞµĞ½Ñ‹ Ğ²: {output_dir}")


if __name__ == "__main__":
    main()

