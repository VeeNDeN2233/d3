# АНАЛИЗ ТЕКУЩЕЙ МОДЕЛИ И ВАРИАНТЫ УЛУЧШЕНИЯ

## Текущая архитектура

**LSTMAutoencoderGPU:**
- **Encoder**: 3 слоя LSTM [128 → 64 → 32]
- **Decoder**: 3 слоя LSTM [32 → 64 → 128 → 75]
- **Latent size**: 32
- **Sequence length**: 30 кадров
- **Input size**: 75 (25 суставов × 3 координаты)

## Проблемы текущей модели

1. **Недостаточная емкость**:
   - Latent size 32 может быть слишком маленьким для 75-мерного пространства
   - Всего 3 слоя в энкодере/декодере

2. **Ограничения LSTM**:
   - Однонаправленная обработка (нет bidirectional)
   - Нет attention механизма для важных кадров
   - Не учитывает пространственные зависимости между суставами

3. **Результаты обучения**:
   - Train loss: ~0.98 (хорошо)
   - Val loss: ~1.69 (приемлемо)
   - Но на реальном видео: ~2.997 (плохо)

## Варианты более сложных моделей

### Вариант 1: Более глубокая LSTM (простой апгрейд)

**Архитектура:**
- Encoder: [256, 128, 64, 32] - 4 слоя
- Decoder: [32, 64, 128, 256, 75] - 5 слоев
- Latent size: 64
- Bidirectional: True

**Плюсы:**
- Легко реализовать
- Улучшит захват паттернов
- Обрабатывает контекст в обе стороны

**Минусы:**
- Все еще ограничения LSTM
- Больше параметров → дольше обучение

### Вариант 2: Transformer-based Autoencoder (современный подход)

**Архитектура:**
- Encoder: Multi-head self-attention (4-8 heads)
- Decoder: Cross-attention + self-attention
- Positional encoding для временных зависимостей
- Latent size: 128

**Плюсы:**
- Лучший захват долгосрочных зависимостей
- Attention показывает важные кадры/суставы
- Современный state-of-the-art подход

**Минусы:**
- Сложнее реализовать
- Больше памяти
- Требует больше данных для обучения

### Вариант 3: Hybrid CNN-LSTM (пространственно-временной)

**Архитектура:**
- CNN Encoder: Обрабатывает пространственные паттерны (суставы)
- LSTM: Обрабатывает временные паттерны (последовательность)
- Latent size: 128

**Плюсы:**
- Учитывает и пространственные, и временные зависимости
- Хорошо для pose estimation

**Минусы:**
- Более сложная архитектура
- Нужно правильно организовать входные данные

### Вариант 4: Variational Autoencoder (VAE)

**Архитектура:**
- Encoder: LSTM → μ, σ (параметры распределения)
- Sampling: z ~ N(μ, σ)
- Decoder: LSTM
- Latent size: 64

**Плюсы:**
- Лучшее латентное пространство
- Регуляризация через KL divergence
- Более плавные переходы

**Минусы:**
- Сложнее обучать
- Может быть избыточно для задачи

### Вариант 5: Bidirectional LSTM + Attention (баланс)

**Архитектура:**
- Encoder: Bidirectional LSTM [256, 128, 64]
- Attention: Multi-head attention на выходе энкодера
- Decoder: LSTM [64, 128, 256, 75]
- Latent size: 64

**Плюсы:**
- Баланс между сложностью и эффективностью
- Attention помогает выделить важные моменты
- Bidirectional улучшает контекст

**Минусы:**
- Средняя сложность реализации

## Рекомендация

**Для начала: Вариант 5 (Bidirectional LSTM + Attention)**

**Причины:**
1. Умеренная сложность - не слишком сложно реализовать
2. Хороший баланс между емкостью и скоростью обучения
3. Attention поможет понять, какие кадры/суставы важны
4. Bidirectional улучшит захват контекста

**Если не поможет:**
- Перейти к Transformer-based (Вариант 2)
- Или Hybrid CNN-LSTM (Вариант 3)

## План действий

1. **Сначала исправить нормализацию** (hip_distance проблема)
2. **Затем улучшить модель** (если проблема останется)
3. **Сравнить результаты** на validation и реальных видео

## Метрики для сравнения

- **Reconstruction error** на validation: должен быть < 1.5
- **Reconstruction error** на реальном видео: должен быть близок к validation
- **Anomaly detection rate** на нормальных видео: < 20%
- **Anomaly detection rate** на аномальных видео: > 80%

